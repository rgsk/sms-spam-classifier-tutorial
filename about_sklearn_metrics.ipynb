{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a performance measurement tool used in machine learning to evaluate the accuracy of a classification model. It is a table that summarizes the predictions made by the model against the actual labels of the data. The confusion matrix consists of four main components:\n",
    "\n",
    "1. True Positive (TP): The number of samples correctly predicted as positive.\n",
    "2. False Positive (FP): The number of samples incorrectly predicted as positive.\n",
    "3. True Negative (TN): The number of samples correctly predicted as negative.\n",
    "4. False Negative (FN): The number of samples incorrectly predicted as negative.\n",
    "\n",
    "A confusion matrix is typically presented in the following format:\n",
    "\n",
    "```\n",
    "               Predicted Positive    Predicted Negative\n",
    "Actual Positive        TP                    FN\n",
    "Actual Negative        FP                    TN\n",
    "```\n",
    "\n",
    "Precision Score:\n",
    "Precision is a performance metric that measures the accuracy of positive predictions made by a model. It is the ratio of true positive predictions to the sum of true positive and false positive predictions. Precision provides an indication of the model's ability to avoid false positives.\n",
    "\n",
    "Precision Score = TP / (TP + FP)\n",
    "\n",
    "Accuracy Score:\n",
    "Accuracy is a performance metric that measures the overall correctness of the predictions made by a model. It is the ratio of the sum of true positive and true negative predictions to the total number of samples. Accuracy provides an indication of the model's ability to classify correctly.\n",
    "\n",
    "Accuracy Score = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "Both precision and accuracy scores are useful in evaluating the performance of a classification model, but they focus on different aspects. Precision is particularly useful in scenarios where false positives are costly, while accuracy provides a general measure of correctness across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8333333333333334\n",
      "Confusion Matrix:\n",
      "[[2 0]\n",
      " [1 3]]\n",
      "Precision Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "\n",
    "# True labels and predicted labels\n",
    "true_labels =       [0, 1, 1, 0, 1, 1]\n",
    "predicted_labels =  [0, 1, 0, 0, 1, 1]\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "print(\"Precision Score:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TN FP\n",
    "FN TP\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
